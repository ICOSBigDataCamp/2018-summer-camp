{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thiese first two lines import the packages you need to scrape\n",
    "import requests #This one manages the requests to the webpage\n",
    "from bs4 import BeautifulSoup #This one imports the beautiful soup library\n",
    " \n",
    "base_url = 'http://www.nytimes.com' #Here is the webpage we are scraping\n",
    "r = requests.get(base_url) #This connrects to the webpage and reads it\n",
    "soup = BeautifulSoup(r.text, \"lxml\")# This is parsing the html and turning it into a soup object\n",
    "\n",
    "#look at the soruce code behind the new york times webpage. it is looking for every time there is a class called story heading\n",
    "# It is then looking for every <a> tag within a story heading class\n",
    "#Then it is saying if there is <a> tag then take the text associated with that a tag and replace the blank space with a new line, and print\n",
    "\n",
    "#If it is not an a tag, just print the storyheading contents\n",
    "#The .strip() removes all of the trailing whitespace characters before and after the string\n",
    "for story_heading in soup.find_all(class_=\"story-heading\"): \n",
    "    if story_heading.a: \n",
    "        print(story_heading.a.text.replace(\"\\n\", \" \").strip())\n",
    "    else: \n",
    "        print(story_heading.contents[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import urllib.request as web\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "base_url = \"http://michiganross.umich.edu/faculty-research/areas-of-study/management-organizations/phd\"\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "names = []\n",
    "for div in soup.find_all('div', {'class' :\"field field-name-field-body field-type-text-long field-label-hidden field-wrapper\"}):\n",
    "    for tr in div.findAll('tr'):\n",
    "        for column in tr.find_all('td'):\n",
    "            for a in column.find_all('a'):\n",
    "                if \".edu\" in a.text:\n",
    "                    continue\n",
    "                    #do nothing. this is an email\n",
    "                else:\n",
    "                    names.append(a.text)\n",
    "print(names)\n",
    "\n",
    "with open(\"MO_Names.txt\", \"w\") as f:\n",
    "    for name in names:\n",
    "        f.write(name)\n",
    "\n",
    "with open(\"MO_Names.txt\", \"r\") as fin:\n",
    "    names_ = fin.readlines()\n",
    "    print(names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import urllib as web\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "clothes=[]\n",
    "prices=[]\n",
    "data_array=[]\n",
    "filename = \"at_data_pull\"+(time.strftime(\"%Y%m%d%H%M\")+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "base_url = 'http://www.anntaylor.com/knits-and-tees/cat70008?pcid=cata00002'\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for div in soup.find_all('div', {'class':'product-wrap'}):\n",
    "    for strong in div.find_all('strong'):\n",
    "    \tclothes_items=strong.text\n",
    "    \tclothes.append(clothes_items)\n",
    "    for span in div.find_all('span',{'itemprop':'price'}):\n",
    "    \tprice_items=span.text\n",
    "    \tprices.append(price_items)\n",
    "\n",
    "for i in range(0, len(clothes)):\n",
    "\ttemp_tuple=(clothes[i], prices[i])\n",
    "\tdata_array.append(temp_tuple)\n",
    "print (temp_tuple[1])\n",
    "#Now lets plop this data into an excel csv\n",
    "with open(filename, \"w\") as f:\n",
    "\tfieldnames = (\"clothes\", \"price\",)\n",
    "\toutput = csv.writer(f, delimiter=\",\", lineterminator='\\n')\n",
    "\toutput.writerow(fieldnames)\t\n",
    "\tfor val in data_array:\n",
    "\t\tval0=val[0]\n",
    "\t\tval1=val[1]\n",
    "\t\toutput.writerow([val0,val1])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
