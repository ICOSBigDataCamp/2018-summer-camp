{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#crawl through imdb\n",
    "\n",
    "#import some handy packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to process movies\n",
    "def parsemovie(soup,writer):\n",
    "  #get movie title, which is string stored under html 'title' tag, & strip out any whitespace at the start/end; then clean out IMDB text\n",
    "  movie = soup.title.string.strip(); movie = movie.encode('utf-8', 'ignore'); movie = movie.decode('utf-8'); movie = re.sub(' - IMDB', '', movie, flags=re.IGNORECASE)\n",
    "  m = re.search('(\\d\\d\\d\\d)', movie)\n",
    "  if m:\n",
    "    year = m.group(1); year=int(year)\n",
    "  else:\n",
    "    year = ''\n",
    "  #get movie url\n",
    "  tag = soup.find(\"link\", rel=\"canonical\"); movie_url = tag[\"href\"]\n",
    "  #find the table that contains the cast list\n",
    "  table = soup.find(\"table\", class_=\"cast_list\")\n",
    "  #find all the rows from the table and create a row-counter\n",
    "  rows = table.find_all(\"tr\"); rowct=0\n",
    "  #print(len(rows),\"rows in the table\")\n",
    "  #iterate thru rows of the table\n",
    "  for row in rows:\n",
    "    #turn the next line one to print each row\n",
    "    #print(row)\n",
    "    #find all the columns in the row\n",
    "    cols = row.find_all(\"td\")\n",
    "    #skip this row if it doesn't contain more than one column\n",
    "    if not (len(cols)>1): continue\n",
    "    #create a list to hold our data\n",
    "    rec = []\n",
    "    #add movie title & url to our list, which we retrieved earlier\n",
    "    rec.append(movie); rec.append(movie_url)\n",
    "    #increment row-counter\n",
    "    rowct+=1\n",
    "    #iterate thru columns to store actors and roles\n",
    "    for col in cols:\n",
    "      #get link stored within the cell (html \"a\" tag)\n",
    "      link = col.find(\"a\")\n",
    "      #try/except allows python to skip a sequence if it doesn't work, rather than fail\n",
    "      try:\n",
    "        #get the text stored under the link\n",
    "        txt = link.text\n",
    "        #proceed if text contains alphanumeric values (note: uses regex package called re)\n",
    "        if re.search('[a-zA-Z0-9]', txt):\n",
    "          #process the text to strip out any whitespace at the start/end, encode it to resolve any unicode issues, and add it to our list\n",
    "          txt = txt.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); rec.append(txt)\n",
    "          #get url from link, which is the value stored under the key 'href', clean it up, & add to the end of our list\n",
    "          url = link[\"href\"]; url = re.sub('\\?ref.+','',url); url='http://www.imdb.com' + url; rec.append(url)\n",
    "      except:\n",
    "        pass\n",
    "    #add the year to our list\n",
    "    rec.append(year)\n",
    "    #send our list to the output handle\n",
    "    writer.writerow(rec)\n",
    "  results = [movie,rowct,movie_url]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to process actors\n",
    "def parseactor(soup,writer):\n",
    "  #get actor name, which is string stored under html 'title' tag, encode, & strip out any whitespace at the start/end; then clean out IMDB text\n",
    "  actor = soup.title.string.strip(); actor = actor.encode('utf-8', 'ignore'); actor = actor.decode('utf-8'); actor = re.sub(' - IMDB','',actor, flags=re.IGNORECASE)\n",
    "  #get actor url\n",
    "  tag = soup.find(\"link\", rel=\"canonical\"); actor_url = tag[\"href\"]\n",
    "  #find the tag at the start of the list of acting credits\n",
    "  div = soup.find(\"div\", class_=\"filmo-category-section\")\n",
    "  #find all the credits nested under this tag\n",
    "  credits = div.find_all(\"div\", class_=\"filmo-row\")\n",
    "  print(len(credits),\"credits in filmography\")\n",
    "  goodct=0\n",
    "  #iterate thru list of credits\n",
    "  for row in credits:\n",
    "    txt = row.text.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); #print(txt)\n",
    "    if '(TV' in txt:\n",
    "      continue\n",
    "    yrsect = row.span.text.strip()\n",
    "    m = re.search('(\\d\\d\\d\\d)', yrsect)\n",
    "    if m:\n",
    "      year = m.group(1); year=int(year)\n",
    "    else:\n",
    "      year = ''\n",
    "    #create a list to hold our data\n",
    "    links = row.find_all(\"a\")\n",
    "    ct=len(links)\n",
    "    if ct != 2:\n",
    "      continue\n",
    "    #iterate thru columns to store actors and roles\n",
    "    ctr=0\n",
    "    for link in links:\n",
    "      txt = link.text.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8');\n",
    "      url = link[\"href\"]; url = re.sub('\\?ref.+','',url); url='http://www.imdb.com' + url\n",
    "      ctr+=1\n",
    "      if ctr==1:\n",
    "        movie_url=url; movie=txt\n",
    "      else:\n",
    "        role_url=url; role=txt\n",
    "    rec = [movie,movie_url,actor,actor_url,role,role_url,year]\n",
    "    #send our list to the output handle\n",
    "    writer.writerow(rec); goodct+=1\n",
    "  results = [actor,goodct,actor_url]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pgs already done\n",
      "1 movies, 15 actors, and 14 characters in dataset\n"
     ]
    }
   ],
   "source": [
    "#input previous work and set up file outputs\n",
    "\n",
    "#input past results, store in dictionary called pgsdone\n",
    "pgsdone = {}\n",
    "with open('imdb-pgs.txt', 'rb') as infile:\n",
    "  for line in infile:\n",
    "    line = line.strip(); pgsdone[line]=1\n",
    "print(len(pgsdone),'pgs already done')\n",
    "\n",
    "#input collected records\n",
    "movies = {}; actors = {}; characters = {}; allurls = {}; ctr=0\n",
    "with open('imdb-recs.txt', 'r') as infile:\n",
    "  for line in infile:\n",
    "    ctr+=1\n",
    "    if ctr==1: continue\n",
    "    (movie,movie_url,actor,actor_url,character,character_url,year) = line.split('\\t')\n",
    "    movies[movie_url]=movie; actors[actor_url]=actor; characters[character_url]=character\n",
    "    allurls[movie_url]=movie; allurls[actor_url]=actor; allurls[character_url]=character\n",
    "print(len(movies),'movies,',len(actors),'actors, and',len(characters),'characters in dataset')\n",
    "\n",
    "#setup three output files (fbak for storing websites collected, fout for storing data scraped, fout2 for recording pages scraped)\n",
    "#note the flags are set to 'ab' for append rather than 'wb' for write, which would erase previously collected data\n",
    "fbak = open('src-pgs.txt', 'ab')\n",
    "fout = open('imdb-recs.txt', 'a', newline='')\n",
    "fout2 = open('imdb-pgs.txt', 'a')\n",
    "#create a handle to write tab-delimited data to the file fout\n",
    "writer = csv.writer(fout, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "working on: http://www.imdb.com/title/tt0451279/   type: movie\n",
      "parsed Wonder Woman (2017)\n",
      "actors found: 15\n",
      "\n",
      "working on: http://www.imdb.com/name/nm2933757/   type: actor\n",
      "19 credits in filmography\n",
      "parsed Gal Gadot\n",
      "credits found: 11\n"
     ]
    }
   ],
   "source": [
    "#iterate through urls to be collected\n",
    "\n",
    "urlct=0\n",
    "for url in allurls:\n",
    "  #skip urls that have already been collected\n",
    "  if url in pgsdone:\n",
    "    continue\n",
    "  #determine the type of url, and skip if don't recognize it (or if character b/c parser for this type of page doesn't exist yet)\n",
    "  if url in movies:\n",
    "    type = 'movie'\n",
    "  elif url in actors:\n",
    "    type = 'actor'\n",
    "  elif url in characters:\n",
    "    type = 'character'\n",
    "    continue\n",
    "  else:\n",
    "    print(\"warn: don't recognize url type for\",url)\n",
    "    continue\n",
    "\n",
    "  #keep track of the number of urls scraped and quit after a certain number (by breaking out of the loop)\n",
    "  urlct+=1\n",
    "  if urlct>2:\n",
    "    break\n",
    "  print(\"\\nworking on:\",url,\"  type:\",type)\n",
    "\n",
    "  #open the url, store it in back up file, and sleep for a random number of seconds\n",
    "  html = urlopen(url).read(); fbak.write(html);\n",
    "  time.sleep(random.randint(4, 10)); #note: sleeping is essential to avoid annoying web servers by calling them too often!\n",
    "\n",
    "  #convert the html into a format that recognizes the structure of the html (and put it into a variable we're calling soup)\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "  if type=='movie':\n",
    "    #send parsed html and output handle to movie parser function\n",
    "    (movie,goodct,movie_url)=parsemovie(soup,writer)\n",
    "    #report how many good rows were found\n",
    "    print('parsed',movie)\n",
    "    print('actors found:',goodct)\n",
    "    fout2.write(movie_url + \"\\n\")\n",
    "  elif type=='actor':\n",
    "    #send parsed html and output handle to actor parser function\n",
    "    (actor,goodct,actor_url)=parseactor(soup,writer)\n",
    "    #report how many good rows were found\n",
    "    print('parsed',actor)\n",
    "    print('credits found:',goodct)\n",
    "    fout2.write(actor_url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#close file outputs\n",
    "fbak.close()\n",
    "fout.close()\n",
    "fout2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
