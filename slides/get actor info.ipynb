{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#practice collecting data from imdb actor pages\n",
    "\n",
    "#import some handy packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup three output files (fbak for storing websites collected, fout for storing data scraped, fout2 for recording pages scraped)\n",
    "fbak = open('src-pgs.txt', 'ab')\n",
    "fout = open('imdb-recs.txt', 'a', newline='')\n",
    "fout2 = open('imdb-pgs.txt', 'a')\n",
    "#create a handle to write tab-delimited data to the file fout\n",
    "writer = csv.writer(fout, delimiter='\\t')\n",
    "#send the header row to this handle\n",
    "#writer.writerow(['movie','movie_url','actor','actor_url','role','role_url','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in html from website\n",
    "#html = urlopen('http://www.imdb.com/title/tt2488496/').read(); fbak.write(html);\n",
    "#time.sleep(random.randint(4, 10));\n",
    "#note: sleeping is essential to avoid annoying web servers by calling them too often!\n",
    "\n",
    "#or copy html into a text file and read it in from there to develop your scraping program first\n",
    "f = open('actorsrc.txt', 'rb'); html = f.read()\n",
    "\n",
    "#convert the html into a format that recognizes the structure of the html (and put it into a variable we're calling soup)\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 credits in filmography\n"
     ]
    }
   ],
   "source": [
    "#***Code below finds all the variables we want about actors and outputs them to a file handle we're calling fout***\n",
    "\n",
    "#get actor name, which is string stored under html 'title' tag, & strip out any whitespace at the start/end; then clean out IMDB text\n",
    "actor = soup.title.string.strip(); actor = actor.encode('utf-8', 'ignore'); actor = actor.decode('utf-8'); actor = re.sub(' - IMDB','',actor, flags=re.IGNORECASE)\n",
    "#get actor url\n",
    "tag = soup.find(\"link\", rel=\"canonical\"); actor_url = tag[\"href\"]\n",
    "#find the tag at the start of the list of acting credits\n",
    "div = soup.find(\"div\", class_=\"filmo-category-section\")\n",
    "#find all the credits nested under this tag\n",
    "credits = div.find_all(\"div\", class_=\"filmo-row\")\n",
    "print(len(credits),\"credits in filmography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodct=0\n",
    "#iterate thru list of credits\n",
    "for row in credits:\n",
    "  txt = row.text.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); #print(txt)\n",
    "  if '(TV' in txt:\n",
    "    continue\n",
    "  yrsect = row.span.text.strip()\n",
    "  m = re.search('(\\d\\d\\d\\d)', yrsect)\n",
    "  if m:\n",
    "    year = m.group(1); year=int(year)\n",
    "  else:\n",
    "    year = ''\n",
    "  links = row.find_all(\"a\")\n",
    "  ct=len(links)\n",
    "  if ct != 2:\n",
    "    continue\n",
    "  #iterate thru columns to store actors and roles\n",
    "  ctr=0\n",
    "  for link in links:\n",
    "    txt = link.text.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); #print(txt)\n",
    "    url = link[\"href\"]; url = re.sub('\\?ref.+','',url); url='http://www.imdb.com' + url;\n",
    "    ctr+=1\n",
    "    if ctr==1:\n",
    "      movie_url=url; movie=txt\n",
    "    else:\n",
    "      role_url=url; role=txt\n",
    "  rec = [movie,movie_url,actor,actor_url,role,role_url,year]\n",
    "  #send our list to the output handle\n",
    "  writer.writerow(rec); goodct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed Gal Gadot\n",
      "credits found: 19   and movies output: 11\n"
     ]
    }
   ],
   "source": [
    "#output url of page we have scraped\n",
    "fout2.write(actor_url + \"\\n\")\n",
    "#report how many good rows were found\n",
    "print('parsed',actor)\n",
    "print('credits found:',len(credits),'  and movies output:',goodct)\n",
    "\n",
    "#close file handles\n",
    "fout.close(); fout2.close(); fbak.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
