{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#practice collecting data from imdb\n",
    "#Code below finds all the variables we want about actors and outputs them to a file handle we're calling fout\n",
    "#note: indentation is a double-space\n",
    "\n",
    "#first, import some handy packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup three output files (fbak for storing websites collected, fout for storing data scraped, fout2 for recording pages scraped)\n",
    "fbak = open('src-pgs.txt', 'wb')\n",
    "fout = open('imdb-recs.txt', 'w', newline='')\n",
    "fout2 = open('imdb-pgs.txt', 'w')\n",
    "#create a handle to write tab-delimited data to the file fout\n",
    "writer = csv.writer(fout, delimiter='\\t')\n",
    "#send the header row to this handle\n",
    "writer.writerow(['movie','movie_url','actor','actor_url','role','role_url','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in the development stage, first copy html into a text file and read it in from there\n",
    "f = open('moviesrc.txt', 'r'); html = f.read()\n",
    "\n",
    "#after perfecting your program, read in html directly from website\n",
    "#html = urlopen('http://www.imdb.com/title/tt0451279/').read(); fbak.write(html);\n",
    "#time.sleep(random.randint(4, 10));\n",
    "#note: sleeping is essential to avoid annoying web servers by calling them too often!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the html into a format that recognizes the structure of the html (and put it into a variable we're calling soup)\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some examples of the power of beautiful soup\n",
    "#see https://www.crummy.com/software/BeautifulSoup/bs4/doc/ for full details\n",
    "\n",
    "#this snippet prints all the links on the page\n",
    "#for link in soup.find_all('a'):\n",
    "#  print(link.get('href'))\n",
    "\n",
    "#and this one prints all the text (note the encoding method is used to avoid errors from unicode characters)\n",
    "#txt = soup.get_text(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get movie title, which is string stored under html 'title' tag, & strip out any whitespace at the start/end; then clean out IMDB text\n",
    "movie = soup.title.string.strip()\n",
    "movie = movie.encode('utf-8', 'ignore'); movie = movie.decode('utf-8')\n",
    "movie = re.sub(' - IMDB', '', movie, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get movie year, which is part of the title string, using regex\n",
    "m = re.search('(\\d\\d\\d\\d)', movie)\n",
    "if m:\n",
    "  year = m.group(1); year=int(year)\n",
    "else:\n",
    "  year = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get movie url\n",
    "tag = soup.find(\"link\", rel=\"canonical\"); movie_url = tag[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 rows in the table\n",
      "rows are stored in a variable w/ data type: <class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "#find the table that contains the cast list\n",
    "table = soup.find(\"table\", class_=\"cast_list\")\n",
    "#find all the rows from the table and create a row-counter\n",
    "rows = table.find_all(\"tr\"); rowct=0\n",
    "print(len(rows),\"rows in the table\")\n",
    "print(\"rows are stored in a variable w/ data type:\",type(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate thru rows of the table\n",
    "for row in rows:\n",
    "  #turn the next line one to print each row\n",
    "  #print row\n",
    "  #find all the columns in the row\n",
    "  cols = row.find_all(\"td\")\n",
    "  #skip this row if it doesn't contain more than one column\n",
    "  if not (len(cols)>1): continue\n",
    "  #create a list to hold our data\n",
    "  rec = []\n",
    "  #add movie title & url to our list, which we retrieved earlier\n",
    "  rec.append(movie); rec.append(movie_url)\n",
    "  #increment row-counter\n",
    "  rowct+=1\n",
    "  #iterate thru columns to store actors and roles\n",
    "  for col in cols:\n",
    "    #get link stored within the cell (html \"a\" tag)\n",
    "    link = col.find(\"a\")\n",
    "    #try/except allows python to skip a sequence if it doesn't work, rather than fail\n",
    "    try:\n",
    "      #get the text stored under the link\n",
    "      txt = link.text\n",
    "      #proceed if text contains alphanumeric values (note: uses regex package called re)\n",
    "      if re.search('[a-zA-Z0-9]', txt):\n",
    "        #process the text to strip out any whitespace at the start/end, encode it to resolve any unicode issues, and add it to our list\n",
    "        txt = txt.strip(); txt = txt.encode('utf-8', 'ignore'); txt = txt.decode('utf-8'); rec.append(txt)\n",
    "        #get url from link, which is the value stored under the key 'href', clean it up, & add to the end of our list\n",
    "        url = link[\"href\"]; url = re.sub('\\?ref.+','',url); url='http://www.imdb.com' + url; rec.append(url)\n",
    "    except:\n",
    "      pass\n",
    "  #add the year to our list\n",
    "  rec.append(year)\n",
    "  #send our list to the output handle\n",
    "  writer.writerow(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed Wonder Woman (2017)\n",
      "actors found: 15\n"
     ]
    }
   ],
   "source": [
    "#report how many good rows were found\n",
    "print('parsed',movie)\n",
    "print('actors found:',rowct)\n",
    "#output url of page we have scraped\n",
    "fout2.write(movie_url + \"\\n\")\n",
    "#close file handles\n",
    "fbak.close(); fout.close(); fout2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
