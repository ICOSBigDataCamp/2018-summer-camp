{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>\n",
    "A Brief Introduction to NLP and the Python NLTK toolkit\n",
    "</h1>\n",
    "<br>\n",
    "\n",
    "<h2 align='center'>\n",
    "<div>Sam Carton</div>\n",
    "\n",
    "<div>School of Information</div>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NLP: Natural Language Processing</h2>\n",
    "\n",
    "Basically the science of developing useful numerical representations of text in order to do various tasks:\n",
    "<ul>\n",
    "<li>Classification</li>\n",
    "<li>Clustering</li>\n",
    "<li>Scientific analysis (i.e. relating derived quantities)</li>\n",
    "<li>Retrieval</li>\n",
    "<li>Matching</li>\n",
    "<li>Lots of other stuff</li>\n",
    "\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Variable-length text to fixed-length numbers/vectors (mostly)</h2>\n",
    "\n",
    "\"I love coffee\" --> [1 0 1 1 0]\n",
    "\n",
    "\"I hate coffee\" --> [1 1 1 0 0]\n",
    "\n",
    "\"I hate that I love coffee\" --> [1 1 2 1 1]\n",
    "\n",
    "\"I love that I hate coffee\" --> [1 1 2 1 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2>NLTK: The most popular general-purpose Python NLP library</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I. Tokenization and bag-of-words representations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'coffee']\n"
     ]
    }
   ],
   "source": [
    "# Almost always the first step: tokenization\n",
    "import nltk\n",
    "\n",
    "sentence = \"I love coffee\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens: ['I', 'love', 'coffee', 'I', 'hate', 'coffee', 'I', 'hate', 'that', 'I', 'love', 'coffee', 'I', 'love', 'that', 'I', 'hate', 'coffee']\n",
      "Vocabulary (sorted and de-duplicated): ['I', 'coffee', 'hate', 'love', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Easiest way to convert from tokens to vectors: bag-of-words representation\n",
    "sentences = [\"I love coffee\",\n",
    "             \"I love coffee\",\n",
    "            \"I hate coffee\",\n",
    "            \"I hate that I love coffee\",\n",
    "            \"I love that I hate coffee\"]\n",
    "\n",
    "#Step 1: tokenize all documents\n",
    "sentence_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "#Step 2: Construct the vocabulary\n",
    "all_tokens = []\n",
    "for token_list in sentence_tokens:\n",
    "    for token in token_list:\n",
    "        all_tokens.append(token)\n",
    "        \n",
    "print(\"All tokens: {}\".format(all_tokens))\n",
    "\n",
    "vocabulary = sorted(list(set(all_tokens)))\n",
    "print(\"Vocabulary (sorted and de-duplicated): {}\".format(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['I', 'coffee', 'hate', 'love', 'that']\n",
      "Sentence vectors:\n",
      "\t\"I love coffee\": [1, 1, 0, 1, 0]\n",
      "\t\"I hate coffee\": [1, 1, 1, 0, 0]\n",
      "\t\"I hate that I love coffee\": [2, 1, 1, 1, 1]\n",
      "\t\"I love that I hate coffee\": [2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Construct a vector the size of the vocabulary for each document, indicating whether that document contains each vocab word in the appropriate column\n",
    "\n",
    "sentence_vectors = []\n",
    "for token_list in sentence_tokens:\n",
    "    sentence_vector = [0]*len(vocabulary)\n",
    "    for token in token_list:\n",
    "        word_index = vocabulary.index(token)\n",
    "        sentence_vector[word_index] += 1\n",
    "    sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "#We should now have an appropriate vector for each sentence\n",
    "print('Vocabulary: {}'.format(vocabulary))\n",
    "print('Sentence vectors:')\n",
    "for sentence,vector in zip(sentences, sentence_vectors):\n",
    "    print('\\t\"{}\": {}'.format(sentence,vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0]\n",
      " [1 1 1 0 0]\n",
      " [2 1 1 1 1]\n",
      " [2 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Scikit-learn library has some stuff that will do this for you\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(lowercase=False,vocabulary=vocabulary,tokenizer=nltk.word_tokenize)\n",
    "matrix = vectorizer.fit_transform(sentences)\n",
    "print(matrix.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>II. What to do with these vector representations?</h2>\n",
    "\n",
    "We're going to do text classification. In particular, we're trying to predict whether comments on the /r/politics subreddit will get more or fewer upvotes than average based on the text of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ignore these functions for now\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "def run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function = None):\n",
    "    '''\n",
    "    Run a machine learning experiment on the provided data. Generate feature representations of input texts\n",
    "    by passing them through the provided function.\n",
    "\n",
    "\n",
    "    :param train_comments: a list of text comments\n",
    "    :param train_labels: a list of numeric 0 or 1 labels\n",
    "    :param test_comments: list of text comments\n",
    "    :param test_labels: list of numeric 0 or 1 labels\n",
    "    :param function: a function that takes in a comment and produces a list of features\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    if process_function:\n",
    "        print('\\nDoing text classification experiment using {} as the processing function for the text'.format(process_function.__name__))\n",
    "    else:\n",
    "        print('\\nDoing text classification experiment without any processing function')\n",
    "\n",
    "    if process_function:\n",
    "        train_vals = [' '.join(process_function(comment)) for comment in train_comments]\n",
    "    else:\n",
    "        train_vals = train_comments\n",
    "\n",
    "    vectorizer = CountVectorizer(lowercase=False)\n",
    "    train_X = vectorizer.fit_transform(train_vals)\n",
    "\n",
    "    if process_function:\n",
    "        test_vals = [' '.join(process_function(comment)) for comment in test_comments]\n",
    "    else:\n",
    "        test_vals = test_comments\n",
    "\n",
    "    test_X = vectorizer.transform(test_vals)\n",
    "\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_X, train_labels)\n",
    "\n",
    "    predicted_test_labels = model.predict(test_X)\n",
    "\n",
    "    acc = accuracy_score(test_labels, predicted_test_labels)\n",
    "    print('Test accuracy score: {:.3f}%'.format(acc))\n",
    "\n",
    "    top_coef_args = np.argsort(np.abs(model.coef_[0]))[-5:]\n",
    "    top_features = [vectorizer.get_feature_names()[x] for x in top_coef_args]\n",
    "    top_coefs = model.coef_[0,top_coef_args]\n",
    "\n",
    "    print('Top 5 most decisive features:')\n",
    "    for feature, coef in reversed(list(zip(top_features,top_coefs))):\n",
    "        print('\\t{}: {:.3f}'.format(feature, coef))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename, comment_column, target_column, filter_values = {}, max_rows = 1000):\n",
    "    print('Reading data from {}'.format(filename))\n",
    "    comments = []\n",
    "    labels = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            include= True\n",
    "            for k,v in filter_values.items():\n",
    "                if not k in row or row[k] != v:\n",
    "                    include = False\n",
    "            if include:\n",
    "                comments.append(row[comment_column])\n",
    "                labels.append(int(row[target_column]))\n",
    "\n",
    "            if len(comments) >= max_rows:\n",
    "                break\n",
    "\n",
    "    return comments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from aww_politics_comments_late_2012_train.csv\n",
      "Reading data from aww_politics_comments_late_2012_test.csv\n",
      "Comments and labels\n",
      "\t0: \"It shows how low the Republican party is when 4 years before the next election Chris Christie is (as...\"\n",
      "\t0: \"What programs are those? I have no insurance, can't afford my own plan and my employer doesn't offer...\"\n",
      "\t0: \"[deleted]...\"\n",
      "\t0: \"Because Papa Johns going out of business = you get health insurance...... how about the fact that 16...\"\n",
      "\t0: \"Ehrmagerd! Look what she's doing to get out of talking about Benghazi! ...\"\n"
     ]
    }
   ],
   "source": [
    "#Read in a couple datasets\n",
    "import numpy as np\n",
    "train_comments, train_labels = read_csv('aww_politics_comments_late_2012_train.csv','body','score', {'subreddit':'politics'})\n",
    "mean_upvotes = np.mean(train_labels)\n",
    "train_labels= [0 if int(x) < mean_upvotes else 1 for x in train_labels]\n",
    "\n",
    "test_comments, test_labels = read_csv('aww_politics_comments_late_2012_test.csv','body','score', {'subreddit':'politics'})\n",
    "test_labels = [0 if int(x) < mean_upvotes else 1 for x in test_labels]\n",
    "\n",
    "\n",
    "print('Comments and labels')\n",
    "for comment, label in list(zip(train_comments, train_labels))[0:5]:\n",
    "    print('\\t{}: \"{}\"'.format(label, comment[0:100]+'...'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing text classification experiment using word_tokenize as the processing function for the text\n",
      "Test accuracy score: 0.766%\n",
      "Top 5 most decisive features:\n",
      "\tplease: 1.287\n",
      "\tSomeone: 0.974\n",
      "\tcorrect: 0.966\n",
      "\twow: 0.860\n",
      "\tRepublican: 0.823\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#Try running a machine learning experiment \n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function = nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III. Stemming</h2>\n",
    "\n",
    "Useful for matching different tenses of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \"run\"; Stemmed: \"run\"\n",
      "Word: \"ran\"; Stemmed: \"ran\"\n",
      "Word: \"running\"; Stemmed: \"run\"\n",
      "Word: \"runner\"; Stemmed: \"runner\"\n",
      "Word: \"runs\"; Stemmed: \"run\"\n",
      "Word: \"runt\"; Stemmed: \"runt\"\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "words = ['run','ran','running','runner','runs','runt']\n",
    "\n",
    "for word in words:\n",
    "    print('Word: \"{}\"; Stemmed: \"{}\"'.format(word, stemmer.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I used to love coffee, but now I don't\n",
      "['i', 'use', 'to', 'love', 'coffe', ',', 'but', 'now', 'i', 'do', \"n't\"]\n",
      "\n",
      "Doing text classification experiment using lowercase_and_stem as the processing function for the text\n",
      "Test accuracy score: 0.751%\n",
      "Top 5 most decisive features:\n",
      "\tpleas: 1.593\n",
      "\trepublican: 1.080\n",
      "\tbullshit: 1.062\n",
      "\tcorrect: 0.987\n",
      "\tone: 0.923\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's try incorporating it into our classification function\n",
    "\n",
    "def lowercase_and_stem(comment):\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [stemmer.stem(x) for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "comment = \"I used to love coffee, but now I don't\"\n",
    "\n",
    "print()\n",
    "print(comment)\n",
    "print(lowercase_and_stem(comment))\n",
    "\n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function = lowercase_and_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IV. Part-of-speech tagging</h2>\n",
    "Sometimes it can be usedful to look at the parts of speech that are used, rather than the words themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ./nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Tokens: ['This', 'is', 'an', 'incisive', 'comment', 'about', 'coffee']\n",
      "Part-of-speech tags: [('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('incisive', 'JJ'), ('comment', 'NN'), ('about', 'IN'), ('coffee', 'NN')]\n",
      "Tags only: ['DT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "#For more sophisticated functions like POS tagging, you have to download extra corpora\n",
    "nltk.download('averaged_perceptron_tagger',download_dir='./nltk_data')\n",
    "\n",
    "comment = 'This is an incisive comment about coffee'\n",
    "tokens = nltk.word_tokenize(comment)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print('Part-of-speech tags: {}'.format(tags))\n",
    "print('Tags only: {}'.format([x[1] for x in tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'IN', 'NN']\n",
      "\n",
      "Doing text classification experiment using pos_tag as the processing function for the text\n",
      "Test accuracy score: 0.787%\n",
      "Top 5 most decisive features:\n",
      "\tSYM: 0.861\n",
      "\tRBS: 0.589\n",
      "\tPDT: 0.512\n",
      "\tFW: -0.487\n",
      "\tNNPS: 0.391\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's try using this in the classification\n",
    "\n",
    "def pos_tag(comment):\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return [x[1] for x in tagged]\n",
    "\n",
    "comment = 'This is yet another trenchant comment about coffee'\n",
    "print(pos_tag(comment))\n",
    "\n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function=pos_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>V. Named-entity recognition</h2>\n",
    "\n",
    "Named entities such as people and organizations can be reconized with reasonable accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vienna', 'Europe', 'Ottoman', 'Mustafa Pasha']\n"
     ]
    }
   ],
   "source": [
    "comment = \"Vienna had some of Europe's earliest coffee shops after its near-conquest by the Ottoman army of Mustafa Pasha\"\n",
    "\n",
    "tokens = nltk.word_tokenize(comment)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "entity_tree = nltk.chunk.ne_chunk(tags)\n",
    "\n",
    "entity_types = ['FACILITY', 'GPE', 'GSP', 'LOCATION', 'ORGANIZATION', 'PERSON']\n",
    "\n",
    "entity_subtrees = [x for x in entity_tree.subtrees() if x.label() in entity_types]\n",
    "entity_strings = [' '.join([y[0] for y in subtree.leaves()]) for subtree in entity_subtrees]\n",
    "\n",
    "print (entity_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jeff', 'Daniels', 'Red Cross International', 'United States']\n",
      "\n",
      "Doing text classification experiment using get_named_entities as the processing function for the text\n",
      "Test accuracy score: 0.794%\n",
      "Top 5 most decisive features:\n",
      "\tSomeone: 1.030\n",
      "\tHouse: -0.995\n",
      "\tDemocratic: 0.935\n",
      "\tDem: 0.929\n",
      "\tOhio: 0.888\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's try incorporating this into our classification process\n",
    "\n",
    "entity_types = ['FACILITY', 'GPE', 'GSP', 'LOCATION', 'ORGANIZATION', 'PERSON']\n",
    "def get_named_entities(comment):\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    entity_tree = nltk.chunk.ne_chunk(tags)\n",
    "    entity_subtrees = [x for x in entity_tree.subtrees() if x.label() in entity_types]\n",
    "    entity_strings = [' '.join([y[0] for y in subtree.leaves()]) for subtree in entity_subtrees]\n",
    "    return entity_strings\n",
    "\n",
    "comment = 'Jeff Daniels made a donation to Red Cross International on behalf of the United States'\n",
    "\n",
    "print(get_named_entities(comment))\n",
    "\n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function=get_named_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> VI. Bigrams </h2>\n",
    "Sometimes you want to look at pairs of tokens instead of just individual tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'yet', 'another', 'comment', 'about', 'coffee']\n",
      "[('This', 'is'), ('is', 'yet'), ('yet', 'another'), ('another', 'comment'), ('comment', 'about'), ('about', 'coffee')]\n"
     ]
    }
   ],
   "source": [
    "comment = 'This is yet another comment about coffee'\n",
    "\n",
    "print (nltk.word_tokenize(comment))\n",
    "\n",
    "print (list(nltk.bigrams(nltk.word_tokenize(comment))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serious_i', 'i_realli', 'realli_need', 'need_coffe', 'coffe_right', 'right_now']\n",
      "\n",
      "Doing text classification experiment using bigram_lowercase_and_stem as the processing function for the text\n",
      "Test accuracy score: 0.788%\n",
      "Top 5 most decisive features:\n",
      "\tbullshit_: 0.771\n",
      "\twhen_you: 0.667\n",
      "\tgood_guy: 0.649\n",
      "\tguy_finland: 0.649\n",
      "\tjust_did: 0.622\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bigramify a couple of our earlier functions and test them out in the classifier\n",
    "def bigram_lowercase_and_stem(comment):\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [stemmer.stem(x) for x in tokens]\n",
    "    return  ['_'.join(x) for x in list(nltk.bigrams(tokens))]\n",
    "\n",
    "comment = 'Seriously I really need coffee right now'\n",
    "print (bigram_lowercase_and_stem(comment))\n",
    "\n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function=bigram_lowercase_and_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRP_VBP', 'VBP_RB', 'RB_JJ', 'JJ_NN', 'NN_RB']\n",
      "\n",
      "Doing text classification experiment using bigram_pos_tag as the processing function for the text\n",
      "Test accuracy score: 0.738%\n",
      "Top 5 most decisive features:\n",
      "\tIN_VBN: 1.435\n",
      "\tTO_NNS: 1.344\n",
      "\tTO_: 1.258\n",
      "\tUH_NN: 1.257\n",
      "\tCC_NNS: -1.202\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bigram_pos_tag(comment):\n",
    "    tokens = nltk.word_tokenize(comment)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return ['_'.join(y) for y in list(nltk.bigrams([x[1] for x in tagged]))]\n",
    "\n",
    "comment = 'I am so tired right now'\n",
    "print(bigram_pos_tag(comment))\n",
    "\n",
    "run_ML_experiment(train_comments, train_labels, test_comments, test_labels, process_function=bigram_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>VII. Conclusion: No clear best representation</h2>\n",
    "\n",
    "The three best representations were: unigrams of parts-of-speech, bigrams of stemmed words, and unigrams of named entities. No super clear lesson to be learned about what causes upvotes and downvotes; more investigation would be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>VIII. Going further: word embeddings</h2>\n",
    "\n",
    "In the past few years, word embeddings have become very popular as a way to build representations of text. They tend to lead to better downstream outcomes than other techniques. \n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
